{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_embedding import BertEmbedding\n",
    "from bert_serving.client import BertClient\n",
    "import pickle\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load data to a csv file\n",
    "def readFile(filename,label):\n",
    "    filename = 'resource/'+filename+'.txt'\n",
    "    with open(filename) as f:\n",
    "        context = f.readlines()\n",
    "    df = pd.DataFrame(columns=['text','label'])\n",
    "    for sms in context:\n",
    "        df=df.append(pd.DataFrame([[sms,label]], columns=['text','label']), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def convertToCSV():\n",
    "    df1 = readFile('ham',0)\n",
    "    df2 = readFile('spam',1)\n",
    "    newDf = df1.append(df2,ignore_index=True)\n",
    "    newDf.to_pickle('resource/raw_data')\n",
    "    \n",
    "    \n",
    "# Simplest bag of words----------------------------------------------------------------------------------\n",
    "def bagOfWords(train, test):\n",
    "    train_data = train.tolist()\n",
    "    test_data = test.tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train_data)\n",
    "\n",
    "    train_matrix = vectorizer.transform(train_data)\n",
    "    test_matrix = vectorizer.transform(test_data)\n",
    "\n",
    "    return train_matrix.toarray(), test_matrix.toarray()\n",
    "\n",
    "# Stupid Doc2Vec-----------------------------------------------------------------------------------------\n",
    "# convert sentences to vectors\n",
    "# https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "def doc2Vec(data,train = True):\n",
    "    if train:\n",
    "        tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "        max_epochs = 100\n",
    "        vec_size = 200\n",
    "        alpha = 0.025\n",
    "        model = Doc2Vec(size=vec_size,\n",
    "                        alpha=alpha, \n",
    "                        min_alpha=0.00025,\n",
    "                        min_count=1,\n",
    "                        dm =1)\n",
    "\n",
    "        model.build_vocab(tagged_data)\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "        #     print('iteration {0}'.format(epoch))\n",
    "            model.train(tagged_data,\n",
    "                        total_examples=model.corpus_count,\n",
    "                        epochs=model.iter)\n",
    "            # decrease the learning rate\n",
    "            model.alpha -= 0.0002\n",
    "            # fix the learning rate, no decay\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        model.save(\"resource/d2v.model\")\n",
    "        result =[]\n",
    "        for sent in data:\n",
    "            test_data = word_tokenize(sent.lower())\n",
    "            v1 = model.infer_vector(test_data)\n",
    "            result.append(v1)\n",
    "        return np.array(result)\n",
    "    else:\n",
    "        model= Doc2Vec.load(\"resource/d2v.model\")\n",
    "        result =[]\n",
    "        for sent in data:\n",
    "            test_data = word_tokenize(sent.lower())\n",
    "            v1 = model.infer_vector(test_data)\n",
    "            result.append(v1)\n",
    "        return np.array(result)\n",
    "\n",
    "\n",
    "# Genius BERT embeddings--------------------------------------------------------------------------------------------\n",
    "\n",
    "#convert words to vectors\n",
    "def word2Vec(filename):\n",
    "    filename ='resource/'+filename+ '.txt'\n",
    "    with open(filename) as f:\n",
    "        sentences = f.readlines()\n",
    "    bert_embedding = BertEmbedding(model='bert_12_768_12', dataset_name='book_corpus_wiki_en_cased')\n",
    "    result = bert_embedding(sentences)\n",
    "    return result\n",
    "\n",
    "#convert sentences to vectors\n",
    "# https://github.com/hanxiao/bert-as-service\n",
    "# run bert-serving-start -model_dir /Users/chang/Downloads/cased_L-24_H-1024_A-16 -num_worker=1  in terminal first\n",
    "def sen2Vec(filename):\n",
    "    filename ='resource/'+filename+ '.txt'\n",
    "    with open(filename) as f:\n",
    "        sentences = f.readlines()\n",
    "    \n",
    "    bc = BertClient(check_length=False)\n",
    "    result = bc.encode(sentences)\n",
    "    return result\n",
    "\n",
    "# load and save data-----------------------------------------------------------------------------------------------\n",
    "def saveAsPickle(data,name):\n",
    "    with open('resource/'+name+'.pickle','wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "def loadAsPickle(name):\n",
    "    with open('resource/'+name+'.pickle','rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    return result\n",
    "\n",
    "# return array of text for data when bert = 0 else bert matrix\n",
    "def loadData(bert = 1):\n",
    "    if bert:\n",
    "        data = loadAsPickle('sent_vec')\n",
    "    else:\n",
    "        with open('resource/'+'all.txt') as f:\n",
    "            data = f.readlines() \n",
    "        data = np.array(data)\n",
    "    label = pd.read_pickle('resource/raw_data')['label'].to_numpy().astype('int')\n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# convertToCSV()\n",
    "matrix = word2Vec('all')\n",
    "# matrix = sen2Vec('all')\n",
    "# saveAsPickle(matrix,'sent_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clfs = [LogisticRegression(), LinearSVC(),KNeighborsClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.975\n",
      "[[20  0]\n",
      " [ 1 19]]\n",
      "0.975\n",
      "[[19  1]\n",
      " [ 0 20]]\n",
      "0.95\n",
      "[[20  0]\n",
      " [ 2 18]]\n",
      "0.925\n",
      "[[19  1]\n",
      " [ 2 18]]\n",
      "0.975\n",
      "[[20  0]\n",
      " [ 1 19]]\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.975\n",
      "[[20  0]\n",
      " [ 1 19]]\n",
      "0.95\n",
      "[[19  1]\n",
      " [ 1 19]]\n",
      "0.95\n",
      "[[20  0]\n",
      " [ 2 18]]\n",
      "0.975\n",
      "[[19  1]\n",
      " [ 0 20]]\n",
      "0.975\n",
      "[[20  0]\n",
      " [ 1 19]]\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "0.925\n",
      "[[18  2]\n",
      " [ 1 19]]\n",
      "0.925\n",
      "[[18  2]\n",
      " [ 1 19]]\n",
      "0.925\n",
      "[[20  0]\n",
      " [ 3 17]]\n",
      "0.9\n",
      "[[19  1]\n",
      " [ 3 17]]\n",
      "1.0\n",
      "[[20  0]\n",
      " [ 0 20]]\n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "data,label = loadData()\n",
    "X = data\n",
    "y = label\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for clf in clfs:\n",
    "    print(clf)\n",
    "    for  test_index,train_index in skf.split(data,label):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print (accuracy_score(y_test,y_pred))\n",
    "        print (confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.5384615384615384\n",
      "[[7 0]\n",
      " [6 0]]\n",
      "0.3076923076923077\n",
      "[[4 2]\n",
      " [7 0]]\n",
      "0.5\n",
      "[[6 0]\n",
      " [6 0]]\n",
      "0.5\n",
      "[[6 0]\n",
      " [6 0]]\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.5384615384615384\n",
      "[[7 0]\n",
      " [6 0]]\n",
      "0.46153846153846156\n",
      "[[0 6]\n",
      " [1 6]]\n",
      "0.5\n",
      "[[6 0]\n",
      " [6 0]]\n",
      "0.5\n",
      "[[6 0]\n",
      " [6 0]]\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "0.5384615384615384\n",
      "[[7 0]\n",
      " [6 0]]\n",
      "0.46153846153846156\n",
      "[[6 0]\n",
      " [7 0]]\n",
      "0.5\n",
      "[[6 0]\n",
      " [6 0]]\n",
      "0.5\n",
      "[[6 0]\n",
      " [6 0]]\n"
     ]
    }
   ],
   "source": [
    "#doc2vec\n",
    "data,label = loadData(0)\n",
    "X = data\n",
    "y = label\n",
    "skf = StratifiedKFold(n_splits = 4)\n",
    "for clf in clfs:\n",
    "    print(clf)\n",
    "    for  train_index,test_index in skf.split(data,label):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train = doc2Vec(X_train,True)\n",
    "        X_test = doc2Vec(X_test,False)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print (accuracy_score(y_test,y_pred))\n",
    "        print (confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.7837837837837838\n",
      "[[18  0]\n",
      " [ 8 11]]\n",
      "0.7567567567567568\n",
      "[[10  9]\n",
      " [ 0 18]]\n",
      "0.7631578947368421\n",
      "[[18  1]\n",
      " [ 8 11]]\n",
      "0.8421052631578947\n",
      "[[16  3]\n",
      " [ 3 16]]\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.8918918918918919\n",
      "[[16  2]\n",
      " [ 2 17]]\n",
      "0.918918918918919\n",
      "[[17  2]\n",
      " [ 1 17]]\n",
      "0.7631578947368421\n",
      "[[18  1]\n",
      " [ 8 11]]\n",
      "0.8421052631578947\n",
      "[[16  3]\n",
      " [ 3 16]]\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "0.8648648648648649\n",
      "[[15  3]\n",
      " [ 2 17]]\n",
      "0.8108108108108109\n",
      "[[15  4]\n",
      " [ 3 15]]\n",
      "0.7105263157894737\n",
      "[[17  2]\n",
      " [ 9 10]]\n",
      "0.8421052631578947\n",
      "[[15  4]\n",
      " [ 2 17]]\n"
     ]
    }
   ],
   "source": [
    "#BoW TfidfVectorizer\n",
    "data,label = loadData(0)\n",
    "X = data\n",
    "y = label\n",
    "skf = StratifiedKFold(n_splits = 4)\n",
    "for clf in clfs:\n",
    "    print(clf)\n",
    "    for  test_index,train_index in skf.split(data,label):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train,X_test = bagOfWords(X_train,X_test)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print (accuracy_score(y_test,y_pred))\n",
    "        print (confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (CogSci3)",
   "language": "python",
   "name": "pycharm-36193b1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
